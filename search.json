[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Jacob Mathew’s Blog",
    "section": "",
    "text": "I come across interesting problems every once in a while. On May 24 2025, I figured why not put them in a blog to show how I approached it.\nConnect with me on GitHub or reach out on LinkedIn."
  },
  {
    "objectID": "projects/wheel-failure-pred/index.html",
    "href": "projects/wheel-failure-pred/index.html",
    "title": "Predicting Wheel Failure on Rail Cars",
    "section": "",
    "text": "In this post, I go into details of how I attempted to solve the problem to predict the likelihood of a train wheel failure in the upcoming months."
  },
  {
    "objectID": "projects/wheel-failure-pred/index.html#introduction",
    "href": "projects/wheel-failure-pred/index.html#introduction",
    "title": "Predicting Wheel Failure on Rail Cars",
    "section": "Introduction",
    "text": "Introduction\nRail wheels are an important asset of the rail inventory, and any type of failure can be a safety hazard, even leading to derailment. Thus, regular maintainance and forward thinking predictive analytics plays a cruicial role in the safe operations of rail cars.\nIn this problem, there are 3 specific types of wheel failures that are being considered. Any other type of failure is grouped into one category ‘Other’. The 3 types are\n\nHigh Impact\nHigh Flange\nThin Flange"
  },
  {
    "objectID": "projects/wheel-failure-pred/index.html#data",
    "href": "projects/wheel-failure-pred/index.html#data",
    "title": "Predicting Wheel Failure on Rail Cars",
    "section": "Data",
    "text": "Data\nTo uniquely identify a wheel for this problem, I looked at the following features: - Equipment (uniquely identified by the equipment number) - Truck (Uniquely identified by the column ‘truck’) - Axle number (Number to represent the axle within a truck) - Side (Left or Right wheel on the axle). - Applied Date (The date when the wheel was installed).\nFor this analysis 4 data sources are combined:\n\nWILD data (Wheel Impact Load Detector)\nWild data consist of wheel impacts measured at different WILD stations spread across the network. It gives information about when the measurement was made, the impact (or load). Different columns give information about the max vertical impact, average vertical impact, dynamic ratio etc.\n\n\nTHD data (Truck Hunting Detector)\nThis dataset contains information indicating when the rail wheels are oscillating excessively. This is measured by hunting index.\n\n\nWPD data (Wheel Profile Detector)\nThis dataset consists information about the measurements of the wheel. It includes flange height, flange width, slope etc.\n\n\nMileage Data\nThis dataset provides information about how many miles each equipment travelled each month. It was also pointed out that mileage information is only available with a 3 month lag, indicating that a sub-model is required to predict where how far each wheel would have travelled during the months that we need to predict.\nWe also have wheel failure data giving information about the month a particular wheel failed and the reason for failure.\nThe failure data contains information for each month. So, once a wheel is installed, we have information about the wheel till it failed or is replaced (all the rows for a wheel would have the column ‘failedin30days’ == 0 until the last row (unless it is replaced without failure).\nThe data for any particular wheel under the WILD, THD, and WPD would only be available if a wheel passes thru the sensor. So it may be possible that we don’t have any data for some months."
  },
  {
    "objectID": "projects/wheel-failure-pred/index.html#methodology",
    "href": "projects/wheel-failure-pred/index.html#methodology",
    "title": "Predicting Wheel Failure on Rail Cars",
    "section": "Methodology",
    "text": "Methodology\nGiven the data is a time series (for each wheel, historic data is available over a period of time), we could look at this problem as a signal processing problem.\n\nData Processing\n\nAggregation of data\nI developed a function that aggregates data at a monthly level. I look at different aggregation methods including, mean, trimmed mean, median, 90 percentile, 10 percentile, etc. My reasoning are as following:\n\nthe objective of this problem is to make a prediction for the upcoming month. The prediction is made at a montly level. So it makes sense to have the datra at the montlhy level.\nThis procedure greatly reduces the memory requirements as we are now only storing the aggregated data (which is useful as I am using a processor with only 8 Gigs of RAM).\n\n\nMethodology for Data Aggregation\nI take raw WPD/THD/WILD time-series and rolling them up to one row per wheel per month, producing a bundle of summary stats (mean/median/std/percentiles/etc.)—plus optional trend (slope)—so those features can be joined to your wheel-level failure table for EDA or modeling.\nThe relevant functions to achieve this output include:\n\n_aggregate_single_group(…) — summarize one (wheel_id, month) group\n\nInputs (via the args tuple):\n\n(entity_id, month): the group key (e.g., a wheel_id and its recordmonth).\ngroup: the pandas DataFrame slice for that key.\nid_col, date_col, agg_date_col: column names for ID, raw timestamp, and month.\nagg_config: dict like {column_name: [list of stats]} that drives what to compute.\n\nWhat it does:\nStarts a result row = {id_col: entity_id, agg_date_col: month}.\nFor each col listed in agg_config (and present in group), it computes - The requested statistics, (e.g., mean, std, count, sum, median) - Percentiles: 10pct and 90pct via np.percentile(…) - Trimmed_mean - Slope via compute_slope(series, group[date_col]) (trend within the month)\nEach stat is stored as f”{col}_{stat}” (e.g., averagevertical_mean).\nReturns the single row dict (one per wheel×month group).\n\naggregate_parallel(…) — run it across all groups with multiprocessing\nInputs:\n\nthe full DataFrame df,\ncolumn names (id_col, date_col, agg_date_col),\nthe agg_config dict\n\n\nWhat it does:\nParses date_col to datetime (errors=“coerce”) and drops rows with missing dates. It then groups the data by [id_col, agg_date_col] (e.g., wheel_id × recordmonth). After that it, builds a task_list of argument tuples for each group. To speed up the process, I used process pool with cpu_count() - 1 workers. Fianlly, it collects all returned row dicts into a list and converts it to a DataFrame—one row per wheel×month with the requested features.\nOutputs: A wide DataFrame where columns follow the pattern _ (e.g., dynamicratio_90pct, maxvertical_std, etc.), plus the key columns (id_col, agg_date_col).\nThis aggregator yields monthly feature vectors per wheel.\nJoining them (e.g., by wheel_id + month alignment logic) gives you explanatory features (WILD/WPD/THD) around the time of failure or censoring, supporting plots of distributions, drift over time, and eventually survival or hazard modeling.\nThis process is repeated for WILD, THD, and WPD data to get montly aggregates.\n\n\n\nProcessing of failure data\nSince failure data is exploded at the monthly level and most of the rows indicate a row with no failure (i.e., failure happens later down the line or the wheel did not fail at all), it made sense to me to reduce the data such that each wheel (uniaquely identified by equiment number, truck, axle, side, applieddate) would be represented by just one row (which is the last row chronologically).\nProcessing pipeline (what the code does)\nGoal: Start from monthly wheel records and end with one row per physical wheel instance (equipment × truck × axle × side × applieddate) that carries a clear failure status, and failure reason.\n\nRead & normalize\n\nLoads the CSV into failure_data_org.\nParses applieddate and recordmonth as datetimes.\nBuilds a composite wheel_id via make_wheel_id(…) (encodes equipment, truck, axle, and side).\nFills missing failurereason values with the sentinel string “not failed” so non-failures are explicit.\n\n\n\nWithin-month de-duplication (per wheel_id × recordmonth)\n\nSorts by wheel_id, recordmonth, failedin30days (descending), then applieddate (ascending) to prioritize failure evidence and earlier installs.\nFor each (wheel_id, recordmonth) group:\nSingle row: keep it.\nNo failures present (failedin30days sum = 0): keep the earliest applieddate in that month.\nFailures present:\nKeep only rows where failedin30days == 1.\nIf exactly one failure row exists: keep it.\nIf multiple failure rows exist:\nRemove trivial duplicates after dropping vendornumbersuppliercode and material.\nIf multiple distinct failure reasons remain, keep them all (they’re meaningfully different).\nOtherwise (same reason), keep the earliest occurrence for that month.\n\nThis logic is implemented in a function called process_failure_data(…) and applied only to groups with more than one row; already-unique groups are set aside to be re-combined later.\n\n\nReassemble partly-cleaned table\n\nrows_to_process selects only the multi-row groups (based on an upstream group_sizes index).\nprocessed_rows applies the grouping logic above.\npartly_cleaned is the union of processed_rows and unique_rows, then sorted by wheel_id and recordmonth.\n\n\n\nAcross-month consolidation (per wheel_id)\nFor each wheel’s timeline (sorted by recordmonth), keeps rows where applieddate changes compared to the next row and drops the last row in each wheel sequence.\nPractical effect: removes repeated monthly observations tied to the same installation event, retaining the boundaries between installation episodes.\n\n\nSplit by outcome and collapse to one row per install\n\nFailed subset: failed = partly_cleaned[failurereason != “not failed”]\nDrops vendornumbersuppliercode and material.\nRemoves exact duplicates.\nComputes time_diff: for each (wheel_id, applieddate_used), the span between the max and min recordmonth. This is a diagnostic of how long that install appears across the monthly logs.\nKeeps the first observation by recordmonth for each (wheel_id, applieddate_used) so one row represents that failed install.\nNot-failed subset: not_failed = partly_cleaned[failurereason == “not failed”]\nKeeps the last observation by recordmonth for each (wheel_id, applieddate_used) so the row represents the most recent non-failure state for that install.\n\n\n\nFinal dataset and derived time in service\n\nConcatenates the collapsed failed and not_failed subsets into clean_failure_data.\nRe-parses dates (safety) and computes time_used = (recordmonth − applieddate) in days, representing time-in-service at the observation.\n\nAfter this we get.\nclean_failure_data — one row per wheel instance (equipment × truck × axle × side × applieddate) carrying: - failurereason (or “not failed”), and - time_used (days in service at the observation month). - failed / not_failed — intermediate tables used to collapse histories to a single row per install. - time_diff — per-install span of months observed (useful for QA on the consolidation).\n\n\nAssumptions captured in the code (not changing behavior—just noting)\nWhen multiple failure rows exist in the same month, vendor/material IDs are ignored for de-duplication to avoid counting trivial differences.\nThe workflow references upstream objects (failure_data, group_sizes, unique_rows, make_wheel_id, applieddate_used) that are expected to be defined earlier in your notebook/script.\nThe across-month filter keeps rows where applieddate changes and drops the last row of each wheel’s sequence; this intentionally emphasizes installation-change boundaries.\n\n\n\nProcessing of mileage data\n\nCleaning & consistency checks\nPurpose.\nMake monthly mileage records coherent before any downstream use: fill NAs, reconcile component miles with totals, and enforce consecutive-month consistency.\nKey steps (what the script does).\n\nLoad & deduplicate. Reads mileage, parses recordmonth, drops exact duplicate rows.\nFill legacy NAs (pre-2025).\nFor rows with recordmonth &lt; 2025-01-01, fills NAs with 0 in: addedmileage, loadedtravelledmiles, emptytravelledmiles.\nPre-clean diagnostics.\nComputes loaded_and_empty = emptytravelledmiles + loadedtravelledmiles and plots it against addedmileage to visualize alignment (or gaps).\nRow-level reconciliation (clean_mileage).\nFor rows where components and totals disagree or are zeroed, applies a set of fixes and tags the action taken in fix_used:\nIf addedmileage == 0 and next month’s totalmileage is available, computes an expected added (next_total − current_total) and tags _added_initFix (no overwrite in this version—diagnostic tag only).\nIf components are both positive and sum exceeds addedmileage, sets addedmileage = loaded + empty and tags _addedFix.\nConsecutive-month consistency (fix_mileage_consistency_consecutive).\nFor each equipmentnumber, enforces (when months are consecutive):\ntotalmileage[i] ≥ totalmileage[i−1] + addedmileage[i].\nIf violated, increases totalmileage[i] to match the expected sum and flags is_mileage_fixed = True.\n\nOnce the cleaning is done, loaded mileage + empty mileage ~ added mileage\n \n\n\nSlopes and changepoints\nPurpose.\nQuantify mileage trends and structural breaks: per-install partmileage slopes and per-equipment totalmileage changepoints/segment slopes.\nKey steps.\n\nVisual audit of missing metadata.\nFor selected equipmentnumber values, plots partmileage timeseries; colors points by applieddate and draws vertical lines at each install date—useful to spot gaps/inconsistencies per (axle, truck, side).\nPer-install partmileage slope.\nFor each (wheel_id, applieddate) sequence:\n\nRegresses partmileage on days since first observation (OLS).\nOutputs coefficient (slope), intercept, r2, and count_good.\nAggregates to a mileage_slope_summary by wheel_id (mean/median/std).\nChangepoints on total mileage (per equipment).\nRuns abrupt changepoint detection on totalmileage (rows before 2025-01-01).\nStores number of breaks, boolean flags (e.g., is_last_seg_flat), and the list of change_months.\nMarks change months in the cleaned table and assigns segment IDs by cumulative sum over change flags.\nSegment-wise totalmileage slopes.\nFits OLS slopes within each (equipmentnumber, segment_id) block.\n\n\nKeep the slopes and a summary filtered to stronger trends (e.g., coefficient &gt; 0.3).\n\n\n\nHow Mileage Progresses over time for one wheel with different applied dates. (Equipment number 721 is shown)\n\n\nSince we know that mileage information has a 3-month lag, I decided to add the projected partmileage, and loaded to empty ratio of the mileage as the features into the model.\n\n\n\n\nJoining Datasets\nAll the datasets do not have all the relevant primary keys to join them together. So to make sure the joins are appropriate, I started with the failure data. As mentioned earlier, failure data is processed such that each row represent the end condition of the wheel as it is replaced (either because it failed or for any other reason). This data contains all the information that can be used to uniquely identify a wheel, as well as the date when the replacement happened. With this information, I can join in the WILD data aggregated (which contains equipment number, truck, axle and side). I can ensure that only the data in the months between the date when the wheel was installed, and the date when the wheel was replaced is to be used)."
  },
  {
    "objectID": "projects/wheel-failure-pred/index.html#observations",
    "href": "projects/wheel-failure-pred/index.html#observations",
    "title": "Predicting Wheel Failure on Rail Cars",
    "section": "Observations",
    "text": "Observations\n\nEDA (WILD vs FAILURE)\nFrom monthly WILD aggregates and the cleaned failure table, I visualize:\nPer-wheel timeseries of WILD features between install and failure (or censoring).\nDistribution comparisons (box/violin) of WILD features at aligned months, by failure class.\n\nData inputs\nwild_agg (monthly WILD aggregates) One row per wheel_id × recordmonth with statistics such as mean, median, 10pct, 90pct for: maxvertical, averagevertical, dynamicvertical, dynamicratio.\nclean_failure_data (wheel installs & outcomes) One row per wheel install (uniquely: equipment × truck × axle × side × applieddate), including: wheel_id, applieddate_used (normalized install date), current observation recordmonth, and failurereason (with “not failed” for non-failures).\n\n\nMerge window & sampling\nA merge on wheel_id joins WILD with installs, then rows are filtered to the window recordmonth_wild ∈ [applieddate_used, recordmonth] so we only keep WILD months observed between install and the analysis month for that wheel.\nFor plots, a small sample of wheels per failure reason is selected (e.g., 15 per class). These samples are for visualization only.\n\n\nTimeseries plots (monthly WILD)\nFor each sampled wheel, for each metric/stat combination: Metrics: maxvertical, averagevertical, dynamicvertical, dynamicratio. Stats: 10pct, 90pct, mean, median. Plot a monthly line from install to outcome with: A vertical line at install and (if applicable) another at failure. If failurereason == “not failed”, the series is censored at “today” for the plot (to show how far we’ve observed it without failure).\nY-axis uses a per-wheel y_max so each plot comfortably fits that wheel’s range. I noticed the max_vertical showed a significant uptick for failure reason = ‘High Impact’ as shown in the figures below.\n \n\n\nResults — Statistical tests on WILD vs. failure\nSample & coverage: We analyzed 45,006 eligible wheel installs (rows in the ratios table after filtering). The class makeup of the data is as follows:\n\nnot failed 30,987 (68.9%),\nhigh impact 5,438 (12.1%),\nthin flange 3,783 (8.4%),\nhigh flange 3,500 (7.8%),\nother 1,298 (2.9%).\n\nWe merged monthly WILD aggregates with the wheel-level table and restricted each sequence to months between install and the analysis month. For end-behavior, we required ≥4 pre-last months per install, then computed:\nEnd mean: mean_maxvertical_90pct_end = mean of the last 3 months.\nEnd escalation: maxvertical_90pct_end_ratio = (mean of last 3 months) ÷ (value 4th from the end).\nVariance check. Levene’s test for the end-window escalation metric (maxvertical_90pct_end_ratio) was highly significant (p = 9.77×10⁻⁷⁵), indicating unequal variances across failure reasons. We therefore used Welch’s ANOVA. The Welch’s ANOVA showed a very large and highly significant difference between the End Means between failure reason (F(4, 5982.56) = 10410.34, p ≈ 0, partial η² = 0.507).\nWe could also visualize it via the violin plot showing the max_vertical (90pct) comparison between high impact failure vs all other failures\n\n\n\nViolin Plot for Max Vertical (High Impact Failure vs Others)\n\n\nFigure references. Box/violin plots and a joint scatter were saved at:\nplots/WILD-EDA/HighImpact vs Others_MaxVerticalEndRatio_Box.png\nplots/WILD-EDA/HighImpact vs Others_MaxVerticalEndRatio_Violin.png\nplots/WILD-EDA/HighImpact vs Others_MaxVerticalEndValuesAnalysis.png\nplots/WILD-EDA/HighImpact vs Others_MaxVerticalEndValues_Violin.png\nplots/WILD-EDA/HighImpact vs Others_MaxVerticalScatterPlot.png\n\n\n\nDistribution of failure data.\nMost frequently occuring ### High Impact."
  },
  {
    "objectID": "posts/dice-game/index.html",
    "href": "posts/dice-game/index.html",
    "title": "Optimal Strategy in a Dice Game",
    "section": "",
    "text": "In this post, I talk about my attempt to find the optimal strategy to maximize the score in a dice game."
  },
  {
    "objectID": "posts/dice-game/index.html#the-game",
    "href": "posts/dice-game/index.html#the-game",
    "title": "Optimal Strategy in a Dice Game",
    "section": "The Game",
    "text": "The Game\nYou roll a fair 10-sided die.\n\nIf you roll a 10, the game ends, and you win nothing.\nIf you roll any other number, that number is added to your score.\nYou are not allowed to stop until your total reaches at least 10.\nOnce your score reaches 10, you are given a choice and you can choose to stop or continue.\n\nHow do you play this game optimally?"
  },
  {
    "objectID": "posts/dice-game/index.html#strategy-1-random-play",
    "href": "posts/dice-game/index.html#strategy-1-random-play",
    "title": "Optimal Strategy in a Dice Game",
    "section": "Strategy 1: Random Play",
    "text": "Strategy 1: Random Play\nI first tried a randomized strategy — once I reached 10, I continued with 50% probability:\n\ndef strategy(res):\n   return random.uniform(0,1) &lt; 0.5\n\ndef roll_die(choices=[1, 2, 3, 4, 5, 6, 7, 8, 9, 10]):\n    return np.random.choice(choices)\n\ndef game():\n    res = 0\n    while res &lt; 10:\n        roll = roll_die()\n        if roll == 10:\n            return 0\n        else:\n            res += roll\n\n    play_again = strategy(res)\n    while play_again:\n        roll = roll_die()\n        if roll == 10:\n            return 0\n        else:\n        play_again = strategy(res)\n            res += roll\n    return res\n\nAfter simulating thousands of games, the average score hovered around 11.6 to 12.0."
  },
  {
    "objectID": "posts/dice-game/index.html#strategy-2-optimized-play",
    "href": "posts/dice-game/index.html#strategy-2-optimized-play",
    "title": "Optimal Strategy in a Dice Game",
    "section": "Strategy 2: Optimized Play",
    "text": "Strategy 2: Optimized Play\nAt this point, I asked myself can I calculate at each turn (after reaching a score of 10) what my score could be after the next roll.\nSay, my score is 10, what possible values can my score take if I roll the die again?\n\n\n\nDice Throw Outcome\nNew Score\nProbability\n\n\n\n\n1\n11\n\\(1/10\\)\n\n\n2\n12\n\\(1/10\\)\n\n\n3\n13\n\\(1/10\\)\n\n\n4\n14\n\\(1/10\\)\n\n\n5\n15\n\\(1/10\\)\n\n\n6\n16\n\\(1/10\\)\n\n\n7\n17\n\\(1/10\\)\n\n\n8\n18\n\\(1/10\\)\n\n\n9\n19\n\\(1/10\\)\n\n\n10\n0\n\\(1/10\\)\n\n\n\nThis means, my expected value at the end of the throw is \\[\n\\sum_{i=11}^{19} \\frac{i}{10} = 13.5\n\\]\nThis is a value greater than my current score. So, it makes sense for me to play the game once more.\nSo I updated my strategy as such.\ndef strategy(res):\n    e = 0\n    for i in range(1, 10):\n        e += (res + i) / 10\n\n    return e &gt; res\nWhat this does is ask the following question. Given my current score, do I expect a higher score if I play again?\nAt first, this confused me — res + i increases as res increases, so why does the gain shrink?\nTurns out, the math reveals:\n\\[E(res)=0.9res+4.5⇒E(res)−res=−0.1res+4.5\\]\nSo as score increases, the value of rolling decreases linearly. Once score reaches 45, the expected gain is 0. Beyond that, rolling again is actually worse than stopping.\nI simulated this strategy using the code below and got a higher range for the confidence of the score (17.25, 17.95)\ndef simulate_game(n=10000):\n    res_list = []\n    for i in range(n):\n        res_list.append(game())\n    return res_list\n\n\ndef find_conf(n=10000, n_conf=10000, conf_per=95):\n    mean_list = []\n    for i in tqdm(range(n_conf), desc=\"num_simulations\"):\n        mean_list.append(np.mean(simulate_game(n)))\n    lower = np.quantile(mean_list, (100 - conf_per) / 200)\n    upper = np.quantile(mean_list, (conf_per + (100 - conf_per) / 2) / 100)\n    return (lower, upper)"
  },
  {
    "objectID": "posts/post-with-code/index.html",
    "href": "posts/post-with-code/index.html",
    "title": "Post With Code",
    "section": "",
    "text": "This is a post with executable code."
  },
  {
    "objectID": "posts/welcome/index.html",
    "href": "posts/welcome/index.html",
    "title": "Welcome To My Blog",
    "section": "",
    "text": "This is the first post in a Quarto blog. Welcome!\n\nSince this post doesn’t specify an explicit image, the first image in the post will be used in the listing page of posts."
  },
  {
    "objectID": "posts/coin-toss-patterns/index.html",
    "href": "posts/coin-toss-patterns/index.html",
    "title": "Expected Value of Number of Attempts to Get Pattern",
    "section": "",
    "text": "In this post, I talk about how to find the expected value of the number of coin tosses needed to find a specific pattern for the first time."
  },
  {
    "objectID": "posts/coin-toss-patterns/index.html#the-setup",
    "href": "posts/coin-toss-patterns/index.html#the-setup",
    "title": "Expected Value of Number of Attempts to Get Pattern",
    "section": "The Setup",
    "text": "The Setup\nYou toss a fair coin until you reach a specific pattern. Example HH. How many tosses would you need on an average to get to this pattern for the first time."
  },
  {
    "objectID": "posts/coin-toss-patterns/index.html#solution",
    "href": "posts/coin-toss-patterns/index.html#solution",
    "title": "Expected Value of Number of Attempts to Get Pattern",
    "section": "Solution",
    "text": "Solution\nTo tackle this problem, I thought about the solution in terms of conditional probability.\nGiven that I have not seen the pattern yet, if the last toss was an H, how many more tosses can I expect before I see the pattern? Call this H. Given that I have not seen the pattern yet, if the last toss was a T, how many more tosses can I expect before I see the pattern? Call this T.\n\\[\nH = 0.5*1 + 0.5*(1+T)\n\\]\nThe explanation of the equation above is, as follows: Given that we have not seen the pattern yet, and we tossed an H, with 0.5 probability we can get the pattern. With another 0.5 probability we need (1+T). 1 because of the T we just rolled, and T is the expected number given the last toss was T.\n\\[\nT = 0.5(1+H) + 0.5(1+T)\n\\]\nWe now have a system of linear equations. On solving this, we get \\[\n\\begin{aligned}\nH = 4\\\\\nT = 6\\\\\nE = 6\n\\end{aligned}\n\\]"
  },
  {
    "objectID": "posts/coin-toss-patterns/index.html#simulate-the-results",
    "href": "posts/coin-toss-patterns/index.html#simulate-the-results",
    "title": "Expected Value of Number of Attempts to Get Pattern",
    "section": "Simulate the Results",
    "text": "Simulate the Results\nimport numpy as np\nimport random\n\n\ndef simulate_game(p=0.5):\n    last_res = None\n    last_but_one_res = None\n    n_trials = 0\n    while not ((last_res == \"H\") & (last_but_one_res == \"H\")):\n        last_but_one_res = last_res\n        if random.uniform(0, 1) &gt; p:\n            last_res = \"H\"\n        else:\n            last_res = \"T\"\n        n_trials += 1\n    return n_trials\n\n\ndef simulate_expected_value(n, p):\n    trials_list = []\n    for i in range(n):\n        trials_list.append(simulate_game(p))\n\n    return np.mean(trials_list)\n\n\ndef find_conf(conf_n, n, p, conf_per=95):\n    res_list = []\n    for i in range(conf_n):\n        res_list.append(simulate_expected_value(n, p))\n    lower = np.quantile(res_list, (100 - conf_per) / 200)\n    upper = np.quantile(res_list, (conf_per + (100 - conf_per) / 2) / 100)\n    return (lower, upper)"
  },
  {
    "objectID": "posts/coin-toss-patterns/index.html#expanding-further",
    "href": "posts/coin-toss-patterns/index.html#expanding-further",
    "title": "Expected Value of Number of Attempts to Get Pattern",
    "section": "Expanding Further",
    "text": "Expanding Further\nWhat if we have\n\nA pattern HHH?\nUnfair dice?\nIs the expected value to get HHH the same as HTH? What about TTH?\nIf the pattern is HH OR HT?\n\nWhat is the probability we get HH before HT?"
  },
  {
    "objectID": "about/index.html",
    "href": "about/index.html",
    "title": "About Me",
    "section": "",
    "text": "Hi, I’m Jacob Mathew — a data scientist and engineer who loves probability puzzles, self-driving systems, and clean code.\nThis blog is where I share technical breakdowns of interesting problems, insights from simulations, and lessons from applying statistics in the real world.\nConnect with me on GitHub or reach out on LinkedIn."
  }
]