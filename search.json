[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Jacob Mathew’s Blog",
    "section": "",
    "text": "I come across interesting problems every once in a while. On May 24 2025, I figured why not put them in a blog to show how I approached it.\nConnect with me on GitHub or reach out on LinkedIn."
  },
  {
    "objectID": "projects/wheel-failure-pred/index.html",
    "href": "projects/wheel-failure-pred/index.html",
    "title": "Predicting Wheel Failure on Rail Cars",
    "section": "",
    "text": "In this post, I go into details of how I attempted to solve the problem to predict the likelihood of a train wheel failure in the upcoming months."
  },
  {
    "objectID": "projects/wheel-failure-pred/index.html#introduction",
    "href": "projects/wheel-failure-pred/index.html#introduction",
    "title": "Predicting Wheel Failure on Rail Cars",
    "section": "Introduction",
    "text": "Introduction\nRail wheels are an important asset of the rail inventory, and any type of failure can be a safety hazard, even leading to derailment. Thus, regular maintainance and forward thinking predictive analytics plays a cruicial role in the safe operations of rail cars.\nIn this problem, there are 3 specific types of wheel failures that are being considered. Any other type of failure is grouped into one category ‘Other’. The 3 types are\n\nHigh Impact\nHigh Flange\nThin Flange"
  },
  {
    "objectID": "projects/wheel-failure-pred/index.html#data",
    "href": "projects/wheel-failure-pred/index.html#data",
    "title": "Predicting Wheel Failure on Rail Cars",
    "section": "Data",
    "text": "Data\nTo uniquely identify a wheel for this problem, I looked at the following features: - Equipment (uniquely identified by the equipment number) - Truck (Uniquely identified by the column ‘truck’) - Axle number (Number to represent the axle within a truck) - Side (Left or Right wheel on the axle). - Applied Date (The date when the wheel was installed).\nFor this analysis 4 data sources are combined:\n\nWILD data (Wheel Impact Load Detector)\nWild data consist of wheel impacts measured at different WILD stations spread across the network. It gives information about when the measurement was made, the impact (or load). Different columns give information about the max vertical impact, average vertical impact, dynamic ratio etc.\n\n\nTHD data (Truck Hunting Detector)\nThis dataset contains information indicating when the rail wheels are oscillating excessively. This is measured by hunting index.\n\n\nWPD data (Wheel Profile Detector)\nThis dataset consists information about the measurements of the wheel. It includes flange height, flange width, slope etc.\n\n\nMileage Data\nThis dataset provides information about how many miles each equipment travelled each month. It was also pointed out that mileage information is only available with a 3 month lag, indicating that a sub-model is required to predict where how far each wheel would have travelled during the months that we need to predict.\n\n\nWheel Failure Data\nWe also have wheel failure data giving information about the month a particular wheel failed and the reason for failure.\nThe failure data contains information for each month. So, once a wheel is installed, we have information about the wheel till it failed or is replaced (all the rows for a wheel would have the column ‘failedin30days’ == 0 until the last row (unless it is replaced without failure).\nThe data for any particular wheel under the WILD, THD, and WPD would only be available if a wheel passes thru the sensor. So it may be possible that we don’t have any data for some months."
  },
  {
    "objectID": "projects/wheel-failure-pred/index.html#data-processing-methodology",
    "href": "projects/wheel-failure-pred/index.html#data-processing-methodology",
    "title": "Predicting Wheel Failure on Rail Cars",
    "section": "Data Processing Methodology",
    "text": "Data Processing Methodology\nGiven the data is a time series (for each wheel, historic data is available over a period of time), we could look at this problem as a signal processing problem.\n\nData Processing\n\nAggregation of data\nI developed a function that aggregates data at a monthly level. I look at different aggregation methods including, mean, trimmed mean, median, 90 percentile, 10 percentile, etc. My reasoning are as following:\n\nthe objective of this problem is to make a prediction for the upcoming month. The prediction is made at a montly level. So it makes sense to have the datra at the montlhy level.\nThis procedure greatly reduces the memory requirements as we are now only storing the aggregated data (which is useful as I am using a processor with only 8 Gigs of RAM).\n\n\nMethodology for Data Aggregation\nI take raw WPD/THD/WILD time-series and rolling them up to one row per wheel per month, producing a bundle of summary stats (mean/median/std/percentiles/etc.)—plus optional trend (slope)—so those features can be joined to your wheel-level failure table for EDA or modeling.\nThe relevant functions to achieve this output include:\n\n_aggregate_single_group(…) — summarize one (wheel_id, month) group\n\nInputs (via the args tuple):\n\n(entity_id, month): the group key (e.g., a wheel_id and its recordmonth).\ngroup: the pandas DataFrame slice for that key.\nid_col, date_col, agg_date_col: column names for ID, raw timestamp, and month.\nagg_config: dict like {column_name: [list of stats]} that drives what to compute.\n\nWhat it does:\nStarts a result row = {id_col: entity_id, agg_date_col: month}.\nFor each col listed in agg_config (and present in group), it computes - The requested statistics, (e.g., mean, std, count, sum, median) - Percentiles: 10pct and 90pct via np.percentile(…) - Trimmed_mean - Slope via compute_slope(series, group[date_col]) (trend within the month)\nEach stat is stored as f”{col}_{stat}” (e.g., averagevertical_mean).\nReturns the single row dict (one per wheel×month group).\n\naggregate_parallel(…) — run it across all groups with multiprocessing\nInputs:\n\nthe full DataFrame df,\ncolumn names (id_col, date_col, agg_date_col),\nthe agg_config dict\n\n\nWhat it does:\nParses date_col to datetime (errors=“coerce”) and drops rows with missing dates. It then groups the data by [id_col, agg_date_col] (e.g., wheel_id × recordmonth). After that it, builds a task_list of argument tuples for each group. To speed up the process, I used process pool with cpu_count() - 1 workers. Fianlly, it collects all returned row dicts into a list and converts it to a DataFrame—one row per wheel×month with the requested features.\nOutputs: A wide DataFrame where columns follow the pattern _ (e.g., dynamicratio_90pct, maxvertical_std, etc.), plus the key columns (id_col, agg_date_col).\nThis aggregator yields monthly feature vectors per wheel.\nJoining them (e.g., by wheel_id + month alignment logic) gives you explanatory features (WILD/WPD/THD) around the time of failure or censoring, supporting plots of distributions, drift over time, and eventually survival or hazard modeling.\nThis process is repeated for WILD, THD, and WPD data to get montly aggregates.\n\n\n\nProcessing of failure data\nSince failure data is exploded at the monthly level and most of the rows indicate a row with no failure (i.e., failure happens later down the line or the wheel did not fail at all), it made sense to me to reduce the data such that each wheel (uniaquely identified by equiment number, truck, axle, side, applieddate) would be represented by just one row (which is the last row chronologically).\n\nProcessing pipeline\nGoal: Start from monthly wheel records and end with one row per physical wheel instance (equipment × truck × axle × side × applieddate) that carries a clear failure status, and failure reason.\n\n\nRead & normalize\n\nLoads the CSV into failure_data_org.\nParses applieddate and recordmonth as datetimes.\nBuilds a composite wheel_id via make_wheel_id(…) (encodes equipment, truck, axle, and side).\nFills missing failurereason values with the sentinel string “not failed” so non-failures are explicit.\n\n\n\nWithin-month de-duplication (per wheel_id × recordmonth)\n\nSorts by wheel_id, recordmonth, failedin30days (descending), then applieddate (ascending) to prioritize failure evidence and earlier installs.\nFor each (wheel_id, recordmonth) group:\nSingle row: keep it.\nNo failures present (failedin30days sum = 0): keep the earliest applieddate in that month.\nFailures present:\nKeep only rows where failedin30days == 1.\nIf exactly one failure row exists: keep it.\nIf multiple failure rows exist:\nRemove trivial duplicates after dropping vendornumbersuppliercode and material.\nIf multiple distinct failure reasons remain, keep them all (they’re meaningfully different).\nOtherwise (same reason), keep the earliest occurrence for that month.\n\nThis logic is implemented in a function called process_failure_data(…) and applied only to groups with more than one row; already-unique groups are set aside to be re-combined later.\n\n\nReassemble partly-cleaned table\n\nrows_to_process selects only the multi-row groups (based on an upstream group_sizes index).\nprocessed_rows applies the grouping logic above.\npartly_cleaned is the union of processed_rows and unique_rows, then sorted by wheel_id and recordmonth.\n\n\n\nAcross-month consolidation (per wheel_id)\nFor each wheel’s timeline (sorted by recordmonth), keeps rows where applieddate changes compared to the next row and drops the last row in each wheel sequence.\nPractical effect: removes repeated monthly observations tied to the same installation event, retaining the boundaries between installation episodes.\n\n\nSplit by outcome and collapse to one row per install\n\nFailed subset: failed = partly_cleaned[failurereason != “not failed”]\nDrops vendornumbersuppliercode and material.\nRemoves exact duplicates.\nComputes time_diff: for each (wheel_id, applieddate_used), the span between the max and min recordmonth. This is a diagnostic of how long that install appears across the monthly logs.\nKeeps the first observation by recordmonth for each (wheel_id, applieddate_used) so one row represents that failed install.\nNot-failed subset: not_failed = partly_cleaned[failurereason == “not failed”]\nKeeps the last observation by recordmonth for each (wheel_id, applieddate_used) so the row represents the most recent non-failure state for that install.\n\n\n\nFinal dataset and derived time in service\n\nConcatenates the collapsed failed and not_failed subsets into clean_failure_data.\nRe-parses dates (safety) and computes time_used = (recordmonth − applieddate) in days, representing time-in-service at the observation.\n\nAfter this we get.\nclean_failure_data — one row per wheel instance (equipment × truck × axle × side × applieddate) carrying: - failurereason (or “not failed”), and - time_used (days in service at the observation month). - failed / not_failed — intermediate tables used to collapse histories to a single row per install. - time_diff — per-install span of months observed (useful for QA on the consolidation).\n\n\nAssumptions captured in the code\n  - When multiple failure rows exist in the same month, vendor/material IDs are ignored for de-duplication to avoid counting trivial differences.\n  - The workflow references upstream objects (failure_data, group_sizes, unique_rows, make_wheel_id, applieddate_used) that are expected to be defined earlier in your notebook/script.\n  - The across-month filter keeps rows where applieddate changes and drops the last row of each wheel’s sequence; this intentionally emphasizes installation-change boundaries.\n\n\n\nProcessing of mileage data\n\nCleaning & consistency checks\nPurpose.\nMake monthly mileage records coherent before any downstream use: fill NAs, reconcile component miles with totals, and enforce consecutive-month consistency.\nBefore this script, the loaded and empty mileage seem to have several inconsistencies with the added miles as shown in the figure below.\n\n\n\nLoaded + Empty Mileage vs Added Mileage before Mileage Data Cleaning\n\n\nKey steps (what the script does).\n\nLoad & deduplicate. Reads mileage, parses recordmonth, drops exact duplicate rows.\nFill legacy NAs (pre-2025).\nFor rows with recordmonth &lt; 2025-01-01, fills NAs with 0 in: addedmileage, loadedtravelledmiles, emptytravelledmiles.\nPre-clean diagnostics.\nComputes loaded_and_empty = emptytravelledmiles + loadedtravelledmiles and plots it against addedmileage to visualize alignment (or gaps).\nRow-level reconciliation (clean_mileage).\nFor rows where components and totals disagree or are zeroed, applies a set of fixes and tags the action taken in fix_used:\nIf addedmileage == 0 and next month’s totalmileage is available, computes an expected added (next_total − current_total) and tags _added_initFix (no overwrite in this version—diagnostic tag only).\nIf components are both positive and sum exceeds addedmileage, sets addedmileage = loaded + empty and tags _addedFix.\nConsecutive-month consistency (fix_mileage_consistency_consecutive).\nFor each equipmentnumber, enforces (when months are consecutive):\ntotalmileage[i] ≥ totalmileage[i−1] + addedmileage[i].\nIf violated, increases totalmileage[i] to match the expected sum and flags is_mileage_fixed = True.\n\nOnce the cleaning is done, loaded mileage + empty mileage ~ added mileage\n\n\n\nLoaded + Empty Mileage vs Added Mileage after Mileage Data Cleaning\n\n\n\n\nSlopes and changepoints\nPurpose.\nQuantify mileage trends and structural breaks: per-install partmileage slopes and per-equipment totalmileage changepoints/segment slopes.\nKey steps.\n\nVisual audit of missing mileage data.\nFor selected equipmentnumber values, plots partmileage timeseries; colors points by applieddate and draws vertical lines at each install date—useful to spot gaps/inconsistencies per (axle, truck, side).\nPer-install partmileage slope.\nFor each (wheel_id, applieddate) sequence:\n\nRegresses partmileage on days since first observation (OLS).\nOutputs coefficient (slope), intercept, r2, and count_good.\nAggregates to a mileage_slope_summary by wheel_id (mean/median/std).\nChangepoints on total mileage (per equipment).\nRuns abrupt changepoint detection on totalmileage (rows before 2025-01-01).\nStores number of breaks, boolean flags (e.g., is_last_seg_flat), and the list of change_months.\nMarks change months in the cleaned table and assigns segment IDs by cumulative sum over change flags.\nSegment-wise totalmileage slopes.\nFits OLS slopes within each (equipmentnumber, segment_id) block.\n\nFor each equipment, find the aggregate of the different slopes calculated after different install dates.\n\nKeep the slopes and a summary filtered to stronger trends (e.g., coefficient &gt; 0.3).\n\n\n\nHow Mileage Progresses over time for one wheel with different applied dates. (Equipment number 721 is shown)\n\n\n\n\nLoaded Mileage vs Added Mileage Analysis\nPurpose. Characterize how loaded miles relate to added miles over time and by equipment; highlight stability, spread, and behavior around structural changes.\nKey steps.\n\nCompute ratio with zero handling. loaded_to_added = loadedtravelledmiles / addedmileage, with a sentinel −1 when addedmileage == 0 (to avoid division by zero).\nPer-equipment summary. For each equipmentnumber, reports mean/median/std/min/max/count and trimmed mean/median (drop two smallest and two largest values) for robustness.\nSpot checks against changepoints. For a random sample of equipment IDs, plots the ratio through time with vertical lines and date labels at the detected change_months.\n\nSince we know that mileage information has a 3-month lag, I decided to add the projected partmileage (using mileage slope calculated), and loaded to empty ratio of the mileage as the features into the model.\n\n\n\n\nJoining Datasets\nAll the datasets do not have all the relevant primary keys to join them together. So to make sure the joins are appropriate, I started with the failure data. As mentioned earlier, failure data is processed such that each row represent the end condition of the wheel as it is replaced (either because it failed or for any other reason). This data contains all the information that can be used to uniquely identify a wheel, as well as the date when the replacement happened. With this information, I can join in the WILD data aggregated (which contains equipment number, truck, axle and side). I can ensure that only the data in the months between the date when the wheel was installed, and the date when the wheel was replaced is to be used)."
  },
  {
    "objectID": "projects/wheel-failure-pred/index.html#observations",
    "href": "projects/wheel-failure-pred/index.html#observations",
    "title": "Predicting Wheel Failure on Rail Cars",
    "section": "Observations",
    "text": "Observations\n\nEDA (WILD vs FAILURE)\nFrom monthly WILD aggregates and the cleaned failure table, I visualize:\nPer-wheel timeseries of WILD features between install and failure (or censoring).\nDistribution comparisons (box/violin) of WILD features at aligned months, by failure class.\n\nData inputs\nwild_agg (monthly WILD aggregates) One row per wheel_id × recordmonth with statistics such as mean, median, 10pct, 90pct for: maxvertical, averagevertical, dynamicvertical, dynamicratio.\nclean_failure_data (wheel installs & outcomes) One row per wheel install (uniquely: equipment × truck × axle × side × applieddate), including: wheel_id, applieddate_used (normalized install date), current observation recordmonth, and failurereason (with “not failed” for non-failures).\n\n\nMerge window & sampling\nA merge on wheel_id joins WILD with installs, then rows are filtered to the window recordmonth_wild ∈ [applieddate_used, recordmonth] so we only keep WILD months observed between install and the analysis month for that wheel.\nFor plots, a small sample of wheels per failure reason is selected (e.g., 15 per class). These samples are for visualization only.\n\n\nTimeseries plots (monthly WILD)\nFor each sampled wheel, for each metric/stat combination: Metrics: maxvertical, averagevertical, dynamicvertical, dynamicratio. Stats: 10pct, 90pct, mean, median. Plot a monthly line from install to outcome with: A vertical line at install and (if applicable) another at failure. If failurereason == “not failed”, the series is censored at “today” for the plot (to show how far we’ve observed it without failure).\nY-axis uses a per-wheel y_max so each plot comfortably fits that wheel’s range. I noticed the max_vertical showed a significant uptick for failure reason = ‘High Impact’ as shown in the figures below.\n \n\n\nResults — Statistical tests on WILD vs. failure\nSample & coverage: We analyzed 45,006 eligible wheel installs (rows in the ratios table after filtering). The class makeup of the data is as follows:\n\nnot failed 30,987 (68.9%),\nhigh impact 5,438 (12.1%),\nthin flange 3,783 (8.4%),\nhigh flange 3,500 (7.8%),\nother 1,298 (2.9%).\n\nWe merged monthly WILD aggregates with the wheel-level table and restricted each sequence to months between install and the analysis month. For end-behavior, we required ≥4 pre-last months per install, then computed:\nEnd mean: mean_maxvertical_90pct_end = mean of the last 3 months.\nEnd escalation: maxvertical_90pct_end_ratio = (mean of last 3 months) ÷ (value 4th from the end).\nVariance check. Levene’s test for the end-window escalation metric (maxvertical_90pct_end_ratio) was highly significant (p = 9.77×10⁻⁷⁵), indicating unequal variances across failure reasons. We therefore used Welch’s ANOVA. The Welch’s ANOVA showed a very large and highly significant difference between the End Means between failure reason (F(4, 5982.56) = 10410.34, p ≈ 0, partial η² = 0.507).\nWe could also visualize it via the violin plot showing the max_vertical (90pct) comparison between high impact failure vs all other failures\n\n\n\nViolin Plot for Max Vertical (High Impact Failure vs Others)\n\n\n\n\n\nEDA (THD vs FAILURE)\nSimilar analysis was done on THD data as well. It was difficult to see any patterns like I saw in max_vertical vs high impact in THD data.\n\n\nEDA (WPD vs FAILURE)\nSince WPD data contains information about flange height and flange thickness, I built a hypothesis that these features would be helpful to determine the failure types of ‘high flange’ and ‘thin flange’. However this was not readily visible via plots as shown below.\n\n\n\nViolin Plot for Flange Height (High Flange Failure vs Others)\n\n\n\n\n\nViolin Plot for Flange Thickness (Thin Flange Failure vs Others)"
  },
  {
    "objectID": "projects/wheel-failure-pred/index.html#model",
    "href": "projects/wheel-failure-pred/index.html#model",
    "title": "Predicting Wheel Failure on Rail Cars",
    "section": "Model",
    "text": "Model\n\nTrain and Validation Split\nPurpose. Create reproducible train/validation datasets from the cleaned wheel table for downstream modeling. The split preserves the class mix of failurereason (stratified) and saves both subsets to versioned folders.\nInputs.\n\nCleaned wheel table (created earlier)\nParameter of interest (val_data_frac = 0.10 → 10% of rows are set aside for validation; 90% for training.)\n\n\nPreprocessing (one row per install).\n\nSort rows by failedin30days (descending).\nDrop duplicates on [“wheel_id”, “applieddate_used”], keeping the first occurrence. Net effect: if multiple rows exist for the same wheel install, the kept row favors one with failedin30days = 1 (due to sorting).\n\n\n\nSplit strategy.\nUse train_test_split with: - test_size = 0.10 (validation fraction from params) - stratify = failurereason (preserve label proportions) - random_state = 42 (reproducible shuffle)\n\n\n\nFeature engineering & alignment\nThe following functions help prepare the data to build the model.\n\nupdate_failure_data(…) — enrich the wheel-level table\nPurpose. Add simple, interpretable covariates to each wheel-month row before joining other data. Features like prior failure count, a loaded-mileage ratio, and an equipment-level mileage slope are added.\nInputs.\n\nfailure_data\nmileage_ratio_summary\ntotalmileage_slope_summary\n\nOutput. Same rows as failure_data, augmented with:\n\nn_prior_failure — cumulative # of earlier events for this wheel_id (per-install history, shifted so it’s strictly prior)\nloaded_ratio — equipment-level trimmed mean of loaded/added mileage\nloadedmileage — partmileage × loaded_ratio (quick proxy for the loaded share)\nmileage_slope — equipment-level mean slope of total mileage growth\n\n\n\ncreate_failure_wild(…) — align WILD months to each wheel’s window\nPurpose. Attach monthly WILD aggregates to each wheel’s observation window—only the months between install and the analysis month for that row.\nInputs.\n\nfailure_data (after the update above)\nwild_agg: monthly WILD aggregates at the wheel level (wheel_id, recordmonth, feature stats)\n\nOutput. A long table with all failure rows replicated across their valid WILD months, containing:\n\nrecordmonth_wild — the WILD calendar month\nAll WILD feature stats for that month\nThe original failure columns for the matching wheel_id\n\nLogic\n\nChunk through wild_agg; in each chunk, rename recordmonth → recordmonth_wild and merge on wheel_id.\nFilter to rows where recordmonth_wild ∈ [applieddate_used, recordmonth] so only post-install, pre-(or at)-outcome months remain.\nConcatenate all chunks.\n\nThis yields a month-by-month panel per wheel that respects the install date and stops at the evaluation month—perfect for building calendar lags next.\n\n\ngenerate_calendar_lags_with_applied_condition(…) — calendar-anchored lags\nPurpose. For each wheel-month row, attach up to k months of historical values (lag1…lagk) for selected features—but never from before the wheel’s install date.\nInputs.\n\ndf: the WILD-enriched failure table (from the function above)\nfeature_cols: list of telemetry columns to lag (e.g., maxvertical_mean, …_90pct, geometry stats, alerts)\nIdentifiers & dates: id_col (usually wheel_id), applied_col (applieddate_used), date_col (recordmonth_wild)\ntrailing_months: number of calendar lags (default 6)\n\nOutput.\nThe same rows as df, plus columns like _lag1, …, _lag6.\nA final alignment filter returns only rows where recordmonth_wild == recordmonth (so each training/validation row is anchored to its label month).\nLogic.\n\nSort by id_col, applied_col, date_col. Keep only the columns needed to compute lags.\nFor each n = 1..k:\n\nCreate a temporary copy where we shift the calendar forward by n months (lag_date = date_col + n months).\nRename  → _lagn and merge back on [id_col, applied_col, lag_date≡date_col].\nThis effectively pulls the value at (t−n) onto the row at month t.\n\nGuardrail: If (t − n) &lt; applieddate_used, set those lag columns to NaN (no pre-install leakage).\nReturn only rows with recordmonth_wild == recordmonth to ensure the features and label are from the same calendar month.\n\nLags are calendar-true (one per month), and not row-shifted. This way they keep the temporal signals. The install-date mask guarantees that early lags don’t peek before a wheel existed. Including applied_col in the merge key keeps separate installs of the same wheel_id from bleeding into each other.\nThe following steps which utilize the above 3 functions are performed to gerneate the training data.\n\nStart with labeled splits (train_failure_data, val_failure_data).\nEnrich with usage/history via update_failure_data(…).\nAttach WILD panels via create_failure_wild(…).\nBuild leakage-safe calendar lags via generate_calendar_lags_with_applied_condition(…) (6 trailing months).\nJoin WPD (wheel) and THD (truck) on the same recordmonth_wild anchor; drop train-speed columns.\n\n\n\n\nDimensionality control\n\nHigh-corr pruning: Compute an absolute correlation matrix over numeric features and drop any feature whose correlation with a previous feature exceeds 0.70.\nAdministrative drops: Remove identifiers, timestamps, and non-predictive keys (e.g., wheel_id, truck_id, equipmentnumber, recordmonth(_wild), vendor/material codes, etc.).\nSave both the full lagged table and the reduced feature matrix for reproducibility.\n\n\n\nModel & tuning\nXGBoost was selected as the modelling approach. Using XGBoost, a multiclass model was fit. The details are given below.\n\nModel: XGBClassifier with objective=“multi:softprob” (multiclass probabilities).\nTarget: failurereason (five classes).\nSearch: RandomizedSearchCV over standard XGB hyperparameters (depth, learning rate, trees, subsample, column subsample, gamma, min_child_weight, L1/L2).\nCV: Stratified K-folds.\nPrimary metric: log loss (mlogloss / neg_log_loss).\nClass weighting: Derives per-class weights from the validation distribution and applies them as sample weights during training to temper class imbalance.\nEvaluation: Predict class probabilities on validation; report log loss.\n\nOn the validation dataset, I achieved a log loss of 0.846. But it must be noted that the validation dataset is not synonymous to the test dataset. The validation set is condensed to form only one row per wheel with one applieddate. This is not the case for test set, as test set has one month per wheel. The result on part of the test data gave 0.207 as the log loss.\n\n\nAssumptions & design choices\nFollowing summarizes the design choices I made to tackle this problem.\n\nNumber of models: Only 1 model, a multi class classifier was chosen. Ensemble model should be explored.\nTemporal leakage guard: Lags are generated post-install and aligned to record months, so the model only sees past telemetry relative to each target month.\nMulticollinearity: A static 0.70 threshold is used for simplicity; SHAP/feature grouping could replace this later.\nClass imbalance: Addressed via sample weights, not by down/up-sampling; preserves the full dataset.\nFeature breadth over specificity: We include a wide panel of WILD/WPD/THD stats first, then let XGB + pruning do the heavy lifting."
  },
  {
    "objectID": "projects/wheel-failure-pred/index.html#limitations-next-steps",
    "href": "projects/wheel-failure-pred/index.html#limitations-next-steps",
    "title": "Predicting Wheel Failure on Rail Cars",
    "section": "Limitations & Next Steps",
    "text": "Limitations & Next Steps\n\nExplainability: Add SHAP summaries to identify the most useful families/lags and to validate domain plausibility.\nTemporal generalization: Consider time-based splits (e.g., train on older months, validate on newer) to mimic prospective performance.\nTarget framing: Multiclass reason is harder than failure vs not; a staged approach (binary failure first, then reason) could lift recall for rarer classes.\nDifferent models for different failurereason: It was found that certain features may be useful for certain defect types. It may be worthwhile to try different models and create one ensemble model as the final one."
  },
  {
    "objectID": "posts/dice-game/index.html",
    "href": "posts/dice-game/index.html",
    "title": "Optimal Strategy in a Dice Game",
    "section": "",
    "text": "In this post, I talk about my attempt to find the optimal strategy to maximize the score in a dice game."
  },
  {
    "objectID": "posts/dice-game/index.html#the-game",
    "href": "posts/dice-game/index.html#the-game",
    "title": "Optimal Strategy in a Dice Game",
    "section": "The Game",
    "text": "The Game\nYou roll a fair 10-sided die.\n\nIf you roll a 10, the game ends, and you win nothing.\nIf you roll any other number, that number is added to your score.\nYou are not allowed to stop until your total reaches at least 10.\nOnce your score reaches 10, you are given a choice and you can choose to stop or continue.\n\nHow do you play this game optimally?"
  },
  {
    "objectID": "posts/dice-game/index.html#strategy-1-random-play",
    "href": "posts/dice-game/index.html#strategy-1-random-play",
    "title": "Optimal Strategy in a Dice Game",
    "section": "Strategy 1: Random Play",
    "text": "Strategy 1: Random Play\nI first tried a randomized strategy — once I reached 10, I continued with 50% probability:\n\ndef strategy(res):\n   return random.uniform(0,1) &lt; 0.5\n\ndef roll_die(choices=[1, 2, 3, 4, 5, 6, 7, 8, 9, 10]):\n    return np.random.choice(choices)\n\ndef game():\n    res = 0\n    while res &lt; 10:\n        roll = roll_die()\n        if roll == 10:\n            return 0\n        else:\n            res += roll\n\n    play_again = strategy(res)\n    while play_again:\n        roll = roll_die()\n        if roll == 10:\n            return 0\n        else:\n        play_again = strategy(res)\n            res += roll\n    return res\n\nAfter simulating thousands of games, the average score hovered around 11.6 to 12.0."
  },
  {
    "objectID": "posts/dice-game/index.html#strategy-2-optimized-play",
    "href": "posts/dice-game/index.html#strategy-2-optimized-play",
    "title": "Optimal Strategy in a Dice Game",
    "section": "Strategy 2: Optimized Play",
    "text": "Strategy 2: Optimized Play\nAt this point, I asked myself can I calculate at each turn (after reaching a score of 10) what my score could be after the next roll.\nSay, my score is 10, what possible values can my score take if I roll the die again?\n\n\n\nDice Throw Outcome\nNew Score\nProbability\n\n\n\n\n1\n11\n\\(1/10\\)\n\n\n2\n12\n\\(1/10\\)\n\n\n3\n13\n\\(1/10\\)\n\n\n4\n14\n\\(1/10\\)\n\n\n5\n15\n\\(1/10\\)\n\n\n6\n16\n\\(1/10\\)\n\n\n7\n17\n\\(1/10\\)\n\n\n8\n18\n\\(1/10\\)\n\n\n9\n19\n\\(1/10\\)\n\n\n10\n0\n\\(1/10\\)\n\n\n\nThis means, my expected value at the end of the throw is \\[\n\\sum_{i=11}^{19} \\frac{i}{10} = 13.5\n\\]\nThis is a value greater than my current score. So, it makes sense for me to play the game once more.\nSo I updated my strategy as such.\ndef strategy(res):\n    e = 0\n    for i in range(1, 10):\n        e += (res + i) / 10\n\n    return e &gt; res\nWhat this does is ask the following question. Given my current score, do I expect a higher score if I play again?\nAt first, this confused me — res + i increases as res increases, so why does the gain shrink?\nTurns out, the math reveals:\n\\[E(res)=0.9res+4.5⇒E(res)−res=−0.1res+4.5\\]\nSo as score increases, the value of rolling decreases linearly. Once score reaches 45, the expected gain is 0. Beyond that, rolling again is actually worse than stopping.\nI simulated this strategy using the code below and got a higher range for the confidence of the score (17.25, 17.95)\ndef simulate_game(n=10000):\n    res_list = []\n    for i in range(n):\n        res_list.append(game())\n    return res_list\n\n\ndef find_conf(n=10000, n_conf=10000, conf_per=95):\n    mean_list = []\n    for i in tqdm(range(n_conf), desc=\"num_simulations\"):\n        mean_list.append(np.mean(simulate_game(n)))\n    lower = np.quantile(mean_list, (100 - conf_per) / 200)\n    upper = np.quantile(mean_list, (conf_per + (100 - conf_per) / 2) / 100)\n    return (lower, upper)"
  },
  {
    "objectID": "posts/post-with-code/index.html",
    "href": "posts/post-with-code/index.html",
    "title": "Post With Code",
    "section": "",
    "text": "This is a post with executable code."
  },
  {
    "objectID": "posts/welcome/index.html",
    "href": "posts/welcome/index.html",
    "title": "Welcome To My Blog",
    "section": "",
    "text": "This is the first post in a Quarto blog. Welcome!\n\nSince this post doesn’t specify an explicit image, the first image in the post will be used in the listing page of posts."
  },
  {
    "objectID": "posts/coin-toss-patterns/index.html",
    "href": "posts/coin-toss-patterns/index.html",
    "title": "Expected Value of Number of Attempts to Get Pattern",
    "section": "",
    "text": "In this post, I talk about how to find the expected value of the number of coin tosses needed to find a specific pattern for the first time."
  },
  {
    "objectID": "posts/coin-toss-patterns/index.html#the-setup",
    "href": "posts/coin-toss-patterns/index.html#the-setup",
    "title": "Expected Value of Number of Attempts to Get Pattern",
    "section": "The Setup",
    "text": "The Setup\nYou toss a fair coin until you reach a specific pattern. Example HH. How many tosses would you need on an average to get to this pattern for the first time."
  },
  {
    "objectID": "posts/coin-toss-patterns/index.html#solution",
    "href": "posts/coin-toss-patterns/index.html#solution",
    "title": "Expected Value of Number of Attempts to Get Pattern",
    "section": "Solution",
    "text": "Solution\nTo tackle this problem, I thought about the solution in terms of conditional probability.\nGiven that I have not seen the pattern yet, if the last toss was an H, how many more tosses can I expect before I see the pattern? Call this H. Given that I have not seen the pattern yet, if the last toss was a T, how many more tosses can I expect before I see the pattern? Call this T.\n\\[\nH = 0.5*1 + 0.5*(1+T)\n\\]\nThe explanation of the equation above is, as follows: Given that we have not seen the pattern yet, and we tossed an H, with 0.5 probability we can get the pattern. With another 0.5 probability we need (1+T). 1 because of the T we just rolled, and T is the expected number given the last toss was T.\n\\[\nT = 0.5(1+H) + 0.5(1+T)\n\\]\nWe now have a system of linear equations. On solving this, we get \\[\n\\begin{aligned}\nH = 4\\\\\nT = 6\\\\\nE = 6\n\\end{aligned}\n\\]"
  },
  {
    "objectID": "posts/coin-toss-patterns/index.html#simulate-the-results",
    "href": "posts/coin-toss-patterns/index.html#simulate-the-results",
    "title": "Expected Value of Number of Attempts to Get Pattern",
    "section": "Simulate the Results",
    "text": "Simulate the Results\nimport numpy as np\nimport random\n\n\ndef simulate_game(p=0.5):\n    last_res = None\n    last_but_one_res = None\n    n_trials = 0\n    while not ((last_res == \"H\") & (last_but_one_res == \"H\")):\n        last_but_one_res = last_res\n        if random.uniform(0, 1) &gt; p:\n            last_res = \"H\"\n        else:\n            last_res = \"T\"\n        n_trials += 1\n    return n_trials\n\n\ndef simulate_expected_value(n, p):\n    trials_list = []\n    for i in range(n):\n        trials_list.append(simulate_game(p))\n\n    return np.mean(trials_list)\n\n\ndef find_conf(conf_n, n, p, conf_per=95):\n    res_list = []\n    for i in range(conf_n):\n        res_list.append(simulate_expected_value(n, p))\n    lower = np.quantile(res_list, (100 - conf_per) / 200)\n    upper = np.quantile(res_list, (conf_per + (100 - conf_per) / 2) / 100)\n    return (lower, upper)"
  },
  {
    "objectID": "posts/coin-toss-patterns/index.html#expanding-further",
    "href": "posts/coin-toss-patterns/index.html#expanding-further",
    "title": "Expected Value of Number of Attempts to Get Pattern",
    "section": "Expanding Further",
    "text": "Expanding Further\nWhat if we have\n\nA pattern HHH?\nUnfair dice?\nIs the expected value to get HHH the same as HTH? What about TTH?\nIf the pattern is HH OR HT?\n\nWhat is the probability we get HH before HT?"
  },
  {
    "objectID": "about/index.html",
    "href": "about/index.html",
    "title": "About Me",
    "section": "",
    "text": "Hi, I’m Jacob Mathew — a data scientist and engineer who loves probability puzzles, self-driving systems, and clean code.\nThis blog is where I share technical breakdowns of interesting problems, insights from simulations, and lessons from applying statistics in the real world.\nConnect with me on GitHub or reach out on LinkedIn."
  }
]